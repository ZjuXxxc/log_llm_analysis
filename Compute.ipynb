{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9b044",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ===== 1ï¸âƒ£ ç”Ÿæˆå¤šè½®å›ç­” =====\n",
    "def roll_llm_responses(problem, n_roll=5):\n",
    "    \"\"\"\n",
    "    å¯¹åŒä¸€ä¸ªæ–‡æœ¬é—®é¢˜ roll å‡ºå¤šç»„å›ç­”ï¼ˆé€‚ç”¨äºçº¯æ–‡æœ¬ Qwenï¼‰\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ§© å½“å‰é—®é¢˜: {problem[:100]}...\")\n",
    "    \n",
    "    # âœ… æ„é€ å¤šè½® messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, a helpful assistant developed by Alibaba Cloud.\"},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"{problem}\\n\\n\"\n",
    "            \"You should first think step by step. \"\n",
    "            \"The reasoning process MUST be enclosed in <think>...</think> tags. \"\n",
    "            \"Then give your final answer in \\\\boxed{{}} format. \"\n",
    "            \"Finally, output a confidence score (1â€“10) in <confidence></confidence> tags.\"\n",
    "        )}\n",
    "    ]\n",
    "\n",
    "    # âœ… è½¬æ¢ä¸º chat æ¨¡æ¿è¾“å…¥\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generations = []\n",
    "    for i in range(n_roll):\n",
    "        print(f\"ç”Ÿæˆç¬¬ {i+1}/{n_roll} ä¸ªå›ç­”...\")\n",
    "        output_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "        # å»æ‰ prompt token éƒ¨åˆ†\n",
    "        output_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, output_ids)\n",
    "        ]\n",
    "        resp = tokenizer.batch_decode(output_ids_trimmed, skip_special_tokens=True)[0]\n",
    "        print(f\"ğŸ§  Roll {i+1} è¾“å‡ºæ‘˜è¦: {resp[:200]}...\\n\")\n",
    "        generations.append(resp)\n",
    "    return generations\n",
    "\n",
    "\n",
    "# ===== 2ï¸âƒ£ è®¡ç®—ç½®ä¿¡åº¦åŒºåˆ† =====\n",
    "def test_llm_confidence_separation(row, n_roll=5, save_dir=\"./logs_llm\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    gt = row[\"answer\"]\n",
    "    problem = row[\"problem\"]\n",
    "\n",
    "    print(f\"\\n===================== ğŸ§© æµ‹è¯•æ ·æœ¬ =====================\")\n",
    "    print(f\"é¢˜ç›®: {problem[:100]}...\")\n",
    "    print(f\"æ ‡å‡†ç­”æ¡ˆ: {gt}\")\n",
    "    print(\"========================================================\")\n",
    "\n",
    "    responses = roll_llm_responses(problem, n_roll=n_roll)\n",
    "    reward_inputs = [{\"response\": r, \"ground_truth\": gt} for r in responses]\n",
    "    scores = compute_score(reward_inputs)  # âœ… ä½¿ç”¨ä½ å·²æœ‰çš„ reward å‡½æ•°\n",
    "\n",
    "    # åˆ†ç±»ç»Ÿè®¡\n",
    "    conf_correct, conf_wrong, record_each = [], [], []\n",
    "    for idx, s in enumerate(scores):\n",
    "        acc = s[\"accuracy\"]\n",
    "        conf = s[\"confidence\"]\n",
    "        rec = {\n",
    "            \"roll_id\": idx + 1,\n",
    "            \"response\": reward_inputs[idx][\"response\"],\n",
    "            \"ground_truth\": gt,\n",
    "            \"confidence\": conf,\n",
    "            \"accuracy\": acc,\n",
    "        }\n",
    "        record_each.append(rec)\n",
    "        if acc == 1.0:\n",
    "            conf_correct.append(conf)\n",
    "        else:\n",
    "            conf_wrong.append(conf)\n",
    "\n",
    "    # è®¡ç®—ç»Ÿè®¡\n",
    "    mean_correct = np.mean(conf_correct) if conf_correct else np.nan\n",
    "    mean_wrong = np.mean(conf_wrong) if conf_wrong else np.nan\n",
    "    delta = mean_correct - mean_wrong if not np.isnan(mean_correct) and not np.isnan(mean_wrong) else np.nan\n",
    "\n",
    "    print(f\"\\nâœ… æ­£ç¡®æ ·æœ¬æ•°: {len(conf_correct)} | âŒ é”™è¯¯æ ·æœ¬æ•°: {len(conf_wrong)}\")\n",
    "    print(f\"âœ… æ­£ç¡®æ ·æœ¬ç½®ä¿¡åº¦å‡å€¼: {mean_correct:.3f}\")\n",
    "    print(f\"âŒ é”™è¯¯æ ·æœ¬ç½®ä¿¡åº¦å‡å€¼: {mean_wrong:.3f}\")\n",
    "    print(f\"Î” (å·®å€¼) = {delta:.3f}\")\n",
    "\n",
    "    if abs(delta) < 0.05 or np.isnan(delta):\n",
    "        print(\"âš ï¸ å·®å¼‚æå°æˆ–æ— æ³•æ¯”è¾ƒ â†’ å¯èƒ½æ˜¯è¿‡æ‹Ÿåˆåˆ° promptã€‚\")\n",
    "    else:\n",
    "        print(\"âœ… ç½®ä¿¡åº¦åŒºåˆ†æ˜æ˜¾ â†’ æ¨¡å‹å¯èƒ½åŸºäº reasoning å­¦ä¹ ä¿¡å¿ƒåº¦ã€‚\")\n",
    "\n",
    "    # ä¿å­˜ JSON æ—¥å¿—\n",
    "    record = {\n",
    "        \"problem\": problem,\n",
    "        \"ground_truth\": gt,\n",
    "        \"n_roll\": n_roll,\n",
    "        \"mean_conf_correct\": mean_correct,\n",
    "        \"mean_conf_wrong\": mean_wrong,\n",
    "        \"delta\": delta,\n",
    "        \"each_roll\": record_each,\n",
    "    }\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{save_dir}/sample_{timestamp}.json\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(record, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"ğŸ“ å·²ä¿å­˜è®°å½•: {filename}\\n\")\n",
    "    return record\n",
    "\n",
    "\n",
    "# ===== 3ï¸âƒ£ æ‰¹é‡è¿è¡Œ =====\n",
    "all_records = []\n",
    "for i in range(len(llm_df)):  # éå†æ•´ä¸ª LLM æ•°æ®é›†\n",
    "    try:\n",
    "        res = test_llm_confidence_separation(llm_df.iloc[i], n_roll=10)\n",
    "        all_records.append(res)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ ·æœ¬ {i} å‡ºé”™: {e}\")\n",
    "\n",
    "# æ±‡æ€» DataFrame\n",
    "summary_rows = [\n",
    "    {\n",
    "        \"problem\": r[\"problem\"][:60],\n",
    "        \"mean_conf_correct\": r[\"mean_conf_correct\"],\n",
    "        \"mean_conf_wrong\": r[\"mean_conf_wrong\"],\n",
    "        \"delta\": r[\"delta\"],\n",
    "        \"n_roll\": r[\"n_roll\"],\n",
    "    }\n",
    "    for r in all_records if r\n",
    "]\n",
    "result_df = pd.DataFrame(summary_rows)\n",
    "print(\"\\n=== ğŸ“Š æ±‡æ€»ç»Ÿè®¡ ===\")\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(\"confidence_summary_llm.csv\", index=False)\n",
    "print(\"âœ… å·²ä¿å­˜ CSV æ–‡ä»¶ï¼šconfidence_summary_llm.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a57009",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def compute_ece(confidences, accuracies, n_bins=10):\n",
    "    \"\"\"Expected Calibration Error\"\"\"\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences >= bins[i]) & (confidences < bins[i + 1])\n",
    "        if np.any(mask):\n",
    "            acc_bin = np.mean(accuracies[mask])\n",
    "            conf_bin = np.mean(confidences[mask])\n",
    "            ece += np.abs(acc_bin - conf_bin) * np.mean(mask)\n",
    "    return ece\n",
    "\n",
    "def analyze_log_folder(folder=\"log_llm\"):\n",
    "    all_confs, all_accs = [], []\n",
    "\n",
    "    results = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "        with open(os.path.join(folder, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        rolls = data.get(\"each_roll\", [])\n",
    "        confs = np.array([r[\"confidence\"] for r in rolls if r.get(\"confidence\") is not None])\n",
    "        accs  = np.array([r[\"accuracy\"]   for r in rolls if r.get(\"accuracy\")   is not None])\n",
    "        \n",
    "        if len(confs) == 0: continue\n",
    "\n",
    "        mean_conf_correct = confs[accs == 1].mean() if np.any(accs == 1) else np.nan\n",
    "        mean_conf_wrong   = confs[accs == 0].mean() if np.any(accs == 0) else np.nan\n",
    "        delta = mean_conf_correct - mean_conf_wrong if not np.isnan(mean_conf_correct) and not np.isnan(mean_conf_wrong) else np.nan\n",
    "        \n",
    "        # æ±‡æ€»å…¨å±€ç»Ÿè®¡\n",
    "        all_confs.extend(confs)\n",
    "        all_accs.extend(accs)\n",
    "\n",
    "        results.append({\n",
    "            \"file\": filename,\n",
    "            \"mean_conf_correct\": mean_conf_correct,\n",
    "            \"mean_conf_wrong\": mean_conf_wrong,\n",
    "            \"delta\": delta,\n",
    "            \"acc_rate\": np.mean(accs)\n",
    "        })\n",
    "\n",
    "    # å…¨å±€æŒ‡æ ‡\n",
    "    all_confs = np.array(all_confs)\n",
    "    all_accs = np.array(all_accs)\n",
    "\n",
    "    global_stats = {\n",
    "        \"global_mean_conf_correct\": all_confs[all_accs == 1].mean(),\n",
    "        \"global_mean_conf_wrong\": all_confs[all_accs == 0].mean(),\n",
    "        \"global_delta\": all_confs[all_accs == 1].mean() - all_confs[all_accs == 0].mean(),\n",
    "        \"global_auc\": roc_auc_score(all_accs, all_confs) if len(np.unique(all_accs)) > 1 else np.nan,\n",
    "        \"global_ece\": compute_ece(all_confs, all_accs),\n",
    "        \"global_brier\": np.mean((all_confs - all_accs) ** 2),\n",
    "        \"global_acc\": np.mean(all_accs)\n",
    "    }\n",
    "\n",
    "    return results, global_stats\n",
    "\n",
    "results, global_stats = analyze_log_folder(\"log_llm\")\n",
    "\n",
    "print(\"=== æ¯ä¸ªæ–‡ä»¶çš„ç»Ÿè®¡ ===\")\n",
    "for r in results:\n",
    "    print(r)\n",
    "\n",
    "print(\"\\n=== å…¨å±€ç»Ÿè®¡ ===\")\n",
    "for k, v in global_stats.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
